# Databricks notebook source
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from delta.tables import DeltaTable

1. Configura√ß√µes

INPUT_TABLE = "main.labdat.silver_sptrans_posicao_geo"  # Entrada da camada Silver B (com lat/lon)
OUTPUT_TABLE = "main.labdat.silver_sptrans_kpis"         # Sa√≠da desta camada

2. Ler a tabela Silver

df_silver = spark.table(INPUT_TABLE)
print(f"Registros lidos de {INPUT_TABLE}: {df_silver.count()}")
display(df_silver.limit(5))

3. Padroniza√ß√£o e enriquecimento geogr√°fico

df_geo = (
    df_silver
    .withColumn("Latitude", F.col("lat"))
    .withColumn("Longitude", F.col("lon"))
    .withColumn("Location", F.concat(F.lit("POINT("), F.col("lon"), F.lit(" "), F.col("lat"), F.lit(")")))
)

4. KPI 1 - Ve√≠culos ativos por linha e hora

df_kpi_active = (
    df_geo
    .withColumn("hour", F.hour("timestamp_utc"))
    .groupBy("date", "hour", "line_code")
    .agg(F.countDistinct("vehicle_id").alias("active_vehicles"))
)

5. KPI 2 - Intervalo m√©dio entre √¥nibus (headway)

window_line = Window.partitionBy("line_code", "vehicle_id").orderBy("timestamp_utc")

df_headway = (
    df_geo
    .withColumn("prev_time", F.lag("timestamp_utc").over(window_line))
    .withColumn("diff_minutes", (F.col("timestamp_utc").cast("long") - F.col("prev_time").cast("long")) / 60)
    .filter(F.col("diff_minutes").isNotNull())
    .groupBy("line_code")
    .agg(F.round(F.avg("diff_minutes"), 2).alias("avg_headway_min"))
)

6. KPI 3 - Velocidade m√©dia (Haversine sem UDF)

window_vehicle = Window.partitionBy("vehicle_id").orderBy("timestamp_utc")

df_speed = (
    df_geo
    .withColumn("prev_lat", F.lag("lat").over(window_vehicle))
    .withColumn("prev_lon", F.lag("lon").over(window_vehicle))
    .withColumn("prev_time", F.lag("timestamp_utc").over(window_vehicle))
    .withColumn("dlat", F.radians(F.col("lat") - F.col("prev_lat")))
    .withColumn("dlon", F.radians(F.col("lon") - F.col("prev_lon")))
    .withColumn(
        "a",
        F.sin(F.col("dlat") / 2) ** 2
        + F.cos(F.radians(F.col("lat"))) * F.cos(F.radians(F.col("prev_lat"))) * F.sin(F.col("dlon") / 2) ** 2
    )
    .withColumn("c", 2 * F.atan2(F.sqrt(F.col("a")), F.sqrt(1 - F.col("a"))))
    .withColumn("dist_m", 6371 * 1000 * F.col("c"))  # dist√¢ncia em metros
    .withColumn("time_diff_s", F.col("timestamp_utc").cast("long") - F.col("prev_time").cast("long"))
    .withColumn("speed_kmh", (F.col("dist_m") / F.col("time_diff_s")) * 3.6)
    .filter(F.col("speed_kmh").isNotNull() & (F.col("speed_kmh") < 80))  # remove outliers
)

df_kpi_speed = (
    df_speed
    .groupBy("line_code")
    .agg(F.round(F.avg("speed_kmh"), 2).alias("avg_speed_kmh"))
)

7. Consolida√ß√£o dos KPIs

df_kpis = (
    df_kpi_active
    .join(df_headway, on="line_code", how="left")
    .join(df_kpi_speed, on="line_code", how="left")
    .withColumn("ingest_ts", F.current_timestamp())
)

display(df_kpis.limit(10))

8. Escrita incremental (Delta MERGE)

if spark.catalog.tableExists(OUTPUT_TABLE):
    print(f"üîÑ Atualizando tabela existente {OUTPUT_TABLE}")
    delta = DeltaTable.forName(spark, OUTPUT_TABLE)
    (
        delta.alias("t")
        .merge(
            df_kpis.alias("s"),
            "t.date = s.date AND t.hour = s.hour AND t.line_code = s.line_code"
        )
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )
else:
    print(f"Criando nova tabela {OUTPUT_TABLE}")
    (
        df_kpis.write
        .format("delta")
        .mode("overwrite")
        .partitionBy("date")
        .option("overwriteSchema", "true")
        .saveAsTable(OUTPUT_TABLE)
    )

print(f"KPIs calculados e salvos em {OUTPUT_TABLE}")

9. Valida√ß√£o final

display(
    spark.sql(f"""
        SELECT date, hour, line_code, active_vehicles, avg_headway_min, avg_speed_kmh
        FROM {OUTPUT_TABLE}
        ORDER BY date DESC, hour DESC
        LIMIT 20
    """)
)
