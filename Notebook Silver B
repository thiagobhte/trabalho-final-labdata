# Databricks notebook source
from pyspark.sql import functions as F
from delta.tables import DeltaTable

1. Configurações

INPUT_TABLE = "main.labdat.silver_sptrans_posicao"
OUTPUT_TABLE = "main.labdat.silver_sptrans_posicao_geo"
DIM_GEOLOC_TABLE = "main.labdat.silver_sptrans_dim_geoloc"

2. Ler Silver existente

df_silver = spark.table(INPUT_TABLE)

print(f"Registros lidos de {INPUT_TABLE}: {df_silver.count()}")
display(df_silver.limit(5))

3. Função UDF: Calcular Geohash sem dependência externa


_base32 = '0123456789bcdefghjkmnpqrstuvwxyz'

def encode_geohash(lat, lon, precision=7):
    if lat is None or lon is None:
        return None
    lat_interval = [-90.0, 90.0]
    lon_interval = [-180.0, 180.0]
    geohash = []
    bits = [16, 8, 4, 2, 1]
    bit = 0
    ch = 0
    even = True

    while len(geohash) < precision:
        if even:
            mid = (lon_interval[0] + lon_interval[1]) / 2
            if lon > mid:
                ch |= bits[bit]
                lon_interval[0] = mid
            else:
                lon_interval[1] = mid
        else:
            mid = (lat_interval[0] + lat_interval[1]) / 2
            if lat > mid:
                ch |= bits[bit]
                lat_interval[0] = mid
            else:
                lat_interval[1] = mid
        even = not even
        if bit < 4:
            bit += 1
        else:
            geohash.append(_base32[ch])
            bit = 0
            ch = 0
    return ''.join(geohash)

geohash_udf = F.udf(lambda lat, lon: encode_geohash(lat, lon, precision=7))

4. Criar dataframe com coluna geohash

df_geo = (
    df_silver
    .withColumn("geohash", geohash_udf(F.col("lat"), F.col("lon")))
    .withColumn("lat_lon", F.concat_ws(", ", F.col("lat"), F.col("lon")))
    .withColumn("ingest_ts", F.current_timestamp())
)

display(df_geo.limit(5))

5. Escrita incremental (MERGE INTO)

if spark.catalog.tableExists(OUTPUT_TABLE):
    print(f"Tabela {OUTPUT_TABLE} já existe → atualizando incrementalmente.")
    delta = DeltaTable.forName(spark, OUTPUT_TABLE)
    (
        delta.alias("t")
        .merge(df_geo.alias("s"),
               "t.vehicle_id = s.vehicle_id AND t.timestamp_utc = s.timestamp_utc")
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )
else:
    print(f"Criando nova tabela {OUTPUT_TABLE} particionada por date.")
    (
        df_geo.write
        .format("delta")
        .mode("overwrite")
        .partitionBy("date")
        .option("overwriteSchema", "true")
        .saveAsTable(OUTPUT_TABLE)
    )

6. Criar dimensão auxiliar com geohash únicos

df_dim_geoloc = (
    df_geo
    .select("geohash", "lat", "lon")
    .dropna(subset=["geohash"])
    .dropDuplicates(["geohash"])
    .withColumn("ingest_ts", F.current_timestamp())
)

(
    df_dim_geoloc.write
    .format("delta")
    .mode("overwrite")
    .saveAsTable(DIM_GEOLOC_TABLE)
)

print("Tabelas criadas e atualizadas com sucesso!")
print(f" - Tabela principal: {OUTPUT_TABLE}")
print(f" - Dimensão geográfica: {DIM_GEOLOC_TABLE}")

7. Validação final

display(
    spark.sql(f"""
        SELECT COUNT(*) AS total_registros, COUNT(DISTINCT geohash) AS total_geohash
        FROM {OUTPUT_TABLE}
    """)
)

display(spark.table(DIM_GEOLOC_TABLE).limit(10))
